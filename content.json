{"posts":[{"title":"Pre-train, Prompt, and Predict A Systematic Survey of Prompting Methods in Natural Language Processing","text":"传统的监督学习采用直接输入与直接输出结果的方式来完成任务。但是基于prompt的学习能够只改变prompt模板就能得到从预测生成的文本中分离出来的结果。这种方式明显更具有通用性，前提是有一个巨大的预训练模型和好的prompt templete。本篇论文介绍了从选择预训练模型到如何组织prompt到微调策略 两个巨大改变第一阶段：纯监督学习：主要依靠特征工程，依靠工程师手工找特征，然后用最大熵回归之类的传统机器学习模型来训练 第二阶段：神经网络的阶段：此时神经网络可以自动找到特征，故而重点变成如何架构一个好的神经网络模型以更好地学习更多的特征 transformer提出之后转向：预训练＋微调 LM：语言模型是一种能够计算出任何给定文本片段（观察到的文本数据）作为一个自然、合理的语言序列出现的概率的系统。 此时重心转向目标工程，具体就是深入思考在特定阶段（预训练或微调）希望模型学习到什么样的能力，根据这个期望，设计或组合能够有效引导模型学习这些能力的训练任务和相应的数学表达（即损失函数）。 到了2021，大家已经不再面对各种特定的任务做微调生成一个新模型，而是用一个大型的pre-train的模型加prompt 做few-shot learning.此时重心来到了prompt engine找到最好的prompt 正式的prompt描述 cloze prompt：z在prompt中间 prefix prompt：z在prompt最后面 可以用虚拟词替代自然语言prompt 答案搜索阶段，我们把所有可能的z都填入，通过LM预测概率 答案映射阶段，我们把很多的输出映射为同一结果 下文会谈论的： 预训练模型trainning objectives标准LM：从左到右的自回归模型，逐个词元预测 去噪目标： CTR（Corrupted Text Reconstruction）：只要求重建损坏的部分，loss不计入其余token FTR（Full Text Reconstruction）：要求重建全文，loss计入所有token 从左到右的自回归模型更适用于prefix prompt,而重建目标的模型更适用于cloze prompt 不同的加噪方式会对模型能力有不同的影响 masking：可以随机，也可以只mask某类词汇以增强模型的某种先验知识 representation direction 从左到右 双向 通常实现方式都是attention masking L2R LMs： decoder-only的泛化性能更好ICML 22的What language model architecture and pretraining objective works best for zero-shot generalization?. 在最大5B参数量、170B token数据量的规模下做了一些列实验，发现用next token prediction预训练的decoder-only模型在各种下游任务上zero-shot泛化性能最好；另外，许多工作表明decoder-only模型的few-shot（也就是上下文学习，in-context learning）泛化能力更强 预训练的难度更低 并没有一个明确的研究表明decoder-only一定更好 masked language models: encoder-only在分类这样的下游任务上表现更好 prefix LMs&amp;encoer-decoder: 兼顾理解和生成能力 由于这两种不适应非生成任务，故而使用prompt可以把非生成任务转化为生成任务 prompt engineeringprompt shape： cloze prompt prefix prompt 离散prompt：实际的，人类可读的promptD1提示挖掘： 给定一组训练数据，包含输入x和输出y 在大型语料库中搜索包含x和y的字符串 找出x与y之间的中间词语或者句子成分中的语法关系 那些存在频率很高的中间词或者语法关系就可以作为模板 如：[x]中间词+中间语法关系[y] D2提示改写： 获得种子prompt（人工手写或者挖掘） 通过不同方式将其改写成多个prompt如：来回翻译、同义词替换、专门的神经网络重写 选择那个能在目标任务上获得最高训练准确率的prompt D3基于梯度的搜索： 方法会一步步地、迭代地尝试不同的词元组合来构成提示。在每一步，它会评估当前词元选择的好坏，并尝试替换或添加能够使模型输出更接近期望目标的词元。 D4提示生成： 将x与y中间填充空，让一个模型来填充这些空 D5提示评分： 获取候选模板库 将x与y填入模板 使用LM对填入后的句子做概率预测 连续prompt 让prompt不一定是自然语言，而是训练出来的能够更好适应下游任务的向量 让模版有自己的可调参数，而不用调整预训练模型本身的参数，更加灵活 C1:前缀微调 前缀在每一层的K和Q都直接由前缀矩阵的参数直接生成而不是依赖原模型的矩阵参数 Lester et al. (2021) 的方法与此类似，他们也是在输入序列前添加特殊标记（tokens）来构成一个模板，并直接调整这些特殊标记的嵌入（embeddings）。与 Li 和 Liang 的方法相比，这种方法引入的参数更少，因为它不会在每个网络层内引入额外的可调参数，而仅仅是调整输入层特殊标记的嵌入。 Tsimpoukelli et al. (2021) 训练了一个视觉编码器，该编码器将图像编码成一个嵌入序列，这个序列可以作为提示（prompt）来引导一个冻结的自回归语言模型生成相应的图像描述。他们证明了由此产生的模型可以在视觉-语言任务（如视觉问答）上实现少样本学习（few-shot learning）。 C2：使用离散提示初始化的调优 具体方法： 这类方法不从零开始学习连续提示，而是利用已经通过离散提示搜索方法创建或发现的提示来初始化连续提示的搜索过程。这样做的好处是，离散提示（通常由人类可读的词语构成）可以为连续提示的优化提供一个更好的起点。 C3：硬提示-软提示混合调优 这类方法不完全依赖于可学习的软提示模板，而是在一个固定的“硬提示”模板（通常由真实词汇构成）中插入一些可调整的嵌入（软提示）。 answer engineeringAnswer Shape主要是粒度区分： Tokens Span：几个token长度–cloze prompt Sentence：–prefix prompt 根据任务选择shape：分类任务选择token或者span，多选项问题回答用sentence Answer Space 无约束空间：包含所有的tokens，生成的就是答案 受约束空间：分类任务、实体识别任务、多项选择 分离答案搜索1. 答案改写 (Answer Paraphrasing) 核心思想： 从一个初始的答案集合（可能是人工设定的）出发，通过“改写”（paraphrasing）技术（比如来回翻译）来扩展这个集合，使其能够覆盖更多语言模型可能输出的、表达相同含义的答案形式。 2. 先剪枝后搜索 (Prune-then-Search) 核心思想： 这类方法通常分两步进行。首先，创建一个初始的、经过“剪枝”的答案空间 Z′（选择频率高的词元或者使用LM生成），这个空间包含一些貌似合理的候选答案词，数量相对较少。然后，再在这个缩小的空间内运行一个更精细的搜索算法（最大似然、逻辑分类器），以挑选出最终的答案词（或答案词集合）。 3. 标签分解 (Label Decomposition) 核心思想： 特别是在处理复杂标签（如关系抽取任务中的关系标签）时，将标签本身分解为其构成词，并将这些构成词的集合作为模型需要生成的答案。 多提示学习方法提示集成 (Prompt ensembling) 是指在推理时针对一个输入使用多个提示 (prompts) 来进行预测的过程。(1) 可以 利用不同 prompt 的互补优势 (leverage the complementary advantages of different prompts)。因为不同的 prompt 可能在处理某些类型的输入或捕捉某些信息时表现更好，集成起来可以取长补短。 (2) 可以 降低 prompt engineering 的成本 (alleviate the cost of prompt engineering)。因为要找到表现最好的那 一个 prompt 是非常困难且耗时的，通过集成多个 prompt，就不用非得找到那个“完美”的 prompt。 (3) 可以 稳定在下游任务上的表现 (stabilize performance on downstream tasks)。集成方法通常能让结果更稳健，不容易受到某个特定 prompt 不稳定的影响。 均匀平均 (Uniform averaging) 当使用多个提示时，组合预测结果最直观的方法是对来自不同提示的概率取平均值 Weighted averaging (加权平均) 为每个prompt分配权重，这个权重往往是事先通过prompt在训练集上的准确率来预先设定，Qin 和 Eisner (2021) 还引入了一种 依赖于数据 (data-dependent) 的加权策略。在这种策略下，在确定不同 prompt 的权重时，也会考虑输入数据与该 prompt 的相关性或输入数据出现在该 prompt 中的概率。 Majority voting (多数投票) 多数投票的基本思想是：每个 prompt 都对输入进行分类，给出一个预测类别，最后统计哪个类别被预测的次数最多（获得“多数票”），就将该类别作为最终的预测结果。 Knowledge distillation (知识蒸馏) 一组深度学习模型的集成（ensemble，即多个模型组合起来）通常可以提升性能。而这种更优越的性能可以通过 知识蒸馏 (knowledge distillation) 的方法转移或“蒸馏”到一个 单一的模型 (single model) 中。 首先用多个 prompt 各自对应的模型组成一个**“教师”集成模型，用它来给无标签数据打标签，**然后训练一个“学生”单一模型去学习这个带标签的数据集，从而把集成模型的知识学到单个模型里。 Prompt ensembling for text generation (用于文本生成的提示词集成) 一种简单的集成方法是使用标准的文本生成技术（比如贪婪搜索、束搜索等），但在生成答案序列中的 下一个词 (next word) (zt) 时，是基于所有 prompt 预测的下一个词概率的 集成（平均）概率 (ensembled probability) 来决定。 Schick 和 Schütze (2020) 的做法是为每一个 prompt fprompt,i(x) 单独训练了一个模型 (separate model)（很可能是对一个基础语言模型进行了微调）。他们首先使用 每个单独训练好的模型分别进行文本生成 (decode generations using each model)，得到各自生成的完整文本序列。然后，他们通过平均 所有模型对某个生成的文本序列的生成概率 (averaging their generation probability across all models) 来对每一个生成的文本序列进行评分。简单来说，不是在生成过程中每一步都平均概率，而是先各自生成完整文本，再对生成的文本根据所有模型的概率进行打分，最后选择分数最高的文本。 Prompt Augmentation (提示词增强) 演示学习 (demonstration learning) 你不再只是简单地给一个 prompt ：“中国的首都是[Z]。” 而是可以在前面加上几个示例，比如：“英国的首都是伦敦。日本的首都是东京。中国的首都是[Z]。” 这样就通过前面的示例向模型展示了回答这类问题的模式。 (1) 示例选择 (Sample Selection): 如何从可能的示例中，选择出最有效、最有帮助的那些？ (2) 示例排序 (Sample Ordering): 选定的示例应该按照什么顺序排列？以及它们应该放在实际 prompt 的前面还是后面，如何组织？ Sample Selection (示例选择) 在 few-shot（少量示例）的场景下，所使用的示例的选择对于模型性能影响非常大。表现可能从在某些任务上接近当前最优水平 (near state-of-the-art accuracy) 到接近随机猜测 (near random guess) 不等。 **Gao 等人 (2021) 和 Liu 等人 (2021a) 使用了 **句子 embedding (sentence embeddings)来选择示例。具体方法是，在句子 embedding 空间中，选择那些与需要模型处理的 输入 (input) 比较接近（距离近）的示例作为演示。这个思路是希望通过选择与当前任务输入在语义或结构上相似的示例来提高效果。 为了衡量预训练语言模型基于指令执行新任务的 泛化能力 (generalization capability)，Mishra 等人 (2021) 在示例中同时提供了 正面示例 (positive samples) 和 反面示例 (negative samples)。反面示例的作用是强调模型应该 避免做的事情 (highlight things to avoid) 或常见的错误类型。 Sample Ordering (示例排序) Lu et al. 发现，表现不佳的提示词顺序往往会导致模型预测分布严重不平衡，因此，他们假设具有较高全局熵的排列可能表现更好，因为它减轻了模型对特定标签的固有偏差。 对于每一个待评估的提示词排列顺序，Lu et al. 将其作为上下文输入到语言模型中，然后让模型对人工开发集中的每个示例进行预测。模型对于每个示例都会输出一个关于所有可能标签的概率分布。基于这些预测的概率分布，他们计算熵。熵在这里作为衡量模型对预测结果不确定性或预测分布均衡性的指标。 Kumar 和 Talukdar (2021) 采取的方法是，他们 搜索 (search) 一个好的训练示例排列顺序作为增强 prompt。此外，他们还学习了一个特殊的 分隔符 token (separator token) 放在每个示例 prompt 之间，以期进一步提升模型性能 Prompt Composition (提示词组合) 对于那些可以基于更基础的 子任务 (subtasks) 进行组合的 可组合任务 (composable tasks)，我们也可以进行 提示词组合 (prompt composition)。这种方法是使用多个 子 prompt (sub-prompts)，每个子 prompt 对应一个子任务，然后基于这些子 prompt 来定义一个 组合 prompt (composite prompt)。 在 关系抽取任务 (relation extraction task) 中（这个任务的目标是抽取两个实体之间的关系），我们可以将这个任务分解成几个子任务，包括识别实体的特征 (identifying the characteristics of entities) 和对实体之间的关系进行分类 (classifying the relationships between entities)。 Prompt Decomposition (提示词分解) 对于需要对同一个样本进行 多次预测 (multiple predictions) 的任务（例如 序列标注 (sequence labeling)，像命名实体识别或词性标注），直接针对整个输入文本 x 定义一个完整的、包含所有预测目标的 prompt 是很有挑战性的。 解决这个问题的一个直观方法是，将这个完整的 prompt 分解成不同的 子 prompt (sub-prompts)，然后 分别回答 (answer separately) 每一个子 prompt。 通过 命名实体识别 (named entity recognition - NER) 任务的例子来说明这个想法。NER 任务的目标是识别输入句子中的所有命名实体。 在 NER 这个例子中，首先会将输入的句子分解成多个 文本片段（span）。然后，可以针对 每一个片段 创建一个 prompt，让模型预测该片段的 实体类型 (entity type)（类型包括人名、地名等，也包括“不是实体”的类别）。 Training Strategies for Prompting Methods (提示词方法的训练策略) 零样本设置 (zero-shot setting)：对于当前关心的这个具体任务来说，没有使用任何训练数据。 数据学习 (full-data learning)，使用相对大量（reasonably large number）的训练样本来训练模型； 少样本学习 (few-shot learning)，只使用非常少量的（very small number）样本来训练模型； 像 Prompt Engineering 这种方法，虽然不训练模型，但常常用任务数据来设计/验证 prompt，这让它在严格意义上并非“纯粹”的零样本。 Parameter Update Methods (参数更新方法) Promptless Fine-tuning (无提示词微调) 在这种策略下，给定一个特定任务的数据集，预训练语言模型（LM）的 所有参数 (all parameters) 或者 部分参数 (some parameters) 会根据下游任务训练样本产生的梯度进行更新（也就是训练）。 这是一种简单、强大且被广泛使用的方法，但是，它可能在 小型数据集 (small datasets) 上出现 过拟合 (overfit) 或者学习不稳定 (not learn stably) 的情况 模型还容易发生 灾难性遗忘 (catastrophic forgetting)。灾难性遗忘是指语言模型在微调后，丢失了它在微调之前（即预训练阶段）所具备的一些能力 Tuning-free Prompting (无微调提示词) 不改变 (without changing) 预训练语言模型的任何参数，仅仅依靠提供的 prompt 来进行预测。 这种方法可以 选择性地 (optionally) 结合使用第 6.2 节介绍的 提示词增强 (prompt augmentation)，即在 prompt 中加入一些带答案的示例来增强输入。这种“无微调提示词”与“提示词增强”相结合的方式，也通常被称为 上下文学习 (in-context learning)（引用了 Brown 等人 2020 年的研究，GPT-3 的能力展示就属于这种 由于 prompt 是唯一指定任务要求的方式，因此需要大量的 人工设计/工程 (heavy engineering) 来精心构造 prompt，才能达到较高的准确率。且上下文学习在示例多时推理慢 Fixed-LM Prompt Tuning (固定语言模型的提示词调优) 仅更新 (updates only) 这些 prompt 的参数 (prompts’ parameters)。它利用从下游训练样本中获取的 监督信号 (supervision signal) 进行训练，同时保持 整个预训练语言模型 (entire pre-trained LM) 的参数 完全不变 (unchanged)。 适用于少样本场景 (suitable in few-shot scenarios)，在数据量较少时也能取得不错的效果。 通常情况下，它的准确率要 优于 (superior accuracy to) 无微调提示词（因为它毕竟进行了一些训练）。 缺点： 不适用于零样本场景 (Not applicable in zero-shot scenarios)，因为它需要训练数据来更新 prompt 参数。 尽管在少样本场景下有效，但在数据量非常大 (large-data settings) 的情况下，由于只训练了很少一部分参数，其 表示能力 (representation power) 可能会受到限制（相比于训练整个大模型）。 仍然需要通过选择 超参数 (hyperparameters)（用于训练 prompt 参数）或初始的 种子 prompt (seed prompts) 来进行一定的 prompt engineering。 训练得到的 prompt 参数（例如一些向量）通常 不可被人直接解释或操纵 (not human-interpretable or manipulable)，不像文本 prompt 那样直观。 Fixed-prompt LM Tuning (固定提示词的语言模型调优) 调优（训练）语言模型的参数 (tunes the parameters of the LM)，这一点与标准的预训练加微调范式是一样的。但它额外地使用了参数 固定不变的 prompt (prompts with fixed parameters) 来指定模型的行为。 优点 (Advantages): 使用 prompt 或答案工程可以更完整地指定任务，从而使得学习过程更加 高效 (efficient learning)，特别是在少样本场景下表现突出。 缺点 (Disadvantages): 仍然需要进行 prompt 或答案工程 (Prompt or answer engineering are still required)，尽管可能不像完全不训练模型的“无微调提示词”那样需要大量设计。 在一个下游任务上通过这种方法微调过的语言模型，可能 不适用于 (may not be effective on) 另一个不同的下游任务（因为它在微调过程中已经偏向了特定的任务，泛化性不如固定模型的 Prompting 方法）。 Prompt+LM Tuning (提示词 + 语言模型调优) 与 prompt 相关的新增参数 (prompt-relevant parameters)（比如软 prompt 的参数）。这些参数可以与预训练模型的 全部或者部分参数 (all or some of the parameters) 一起进行 联合微调（训练）(fine-tuned together)。 优点 (Advantages): 这是所有方法中 表达能力最强 (most expressive method) 的一种，因为它同时训练了模型的许多参数和 prompt 相关的参数，具有最大的灵活性。 可能最适合在 高数据量 (high-data settings) 的情况下使用，能够充分利用大量数据。 缺点 (Disadvantages): 需要训练和存储模型的 所有参数 (all parameters)（或者大部分参数），这通常意味着需要更多的计算资源和存储空间。 在 小型数据集 (small datasets) 上进行训练时，由于参数量大，可能容易发生 过拟合 (May overfit)。 Applications (应用)Knowledge Probing (知识探测)Factual Probing (事实性探测) 量化预训练语言模型内部的表示（representations）中包含了多少 事实性知识 (factual knowledge)。预训练模型的参数通常是 固定不变 (fixed) 的。通过将原始输入转换成第 2.2 节定义的 完形填空式 prompt (cloze prompt) 来检索知识 Linguistic Probing (语言学探测) 除了事实性知识，大规模的预训练也使得语言模型能够处理各种 语言学现象 (linguistic phenomena)，比如类比、否定、对语义角色的敏感性、语义相似度、俚语（或行话）理解、以及对稀有词语的理解等。 Classification-based Tasks (基于分类的任务)Text Classification (文本分类) 现有的大多数工作都是在 少样本设置 (few-shot setting) 下，探索使用 “固定提示词的语言模型调优”策略 (§7.2.4 中定义的方法) 来进行文本分类的有效性。这意味着他们使用固定的 prompt 模板，并微调语言模型的部分或全部参数 Natural Language Inference (NLI) (自然语言推理) 自然语言推理（NLI）任务的目标是预测给定两个句子之间的关系（例如，一个句子是否蕴含另一个句子 entailment）。 在 prompt engineering 方面，研究人员主要关注于在 少样本学习设置 (few-shot learning setting) 下进行 模板搜索 (template search)，并且 prompt 中待预测的槽位 [Z] 的 答案空间 (answer space)（即可能填充的词语）通常是从词汇表中 手动预先选择 (manually pre-selected) 出来，并与最终的类别标签（蕴含、矛盾、中立等）进行映射。 Information Extraction (信息抽取)Relation Extraction (关系抽取) 关系抽取任务是预测句子中两个实体之间关系的任务。 (1) 标签空间更大 (The larger label space): 关系抽取的类别数量通常比二分类任务多很多（例如，关系抽取有 80 种关系，而二分类情感分析只有 2 种情感），这使得 答案工程 (answer engineering) 更困难（需要将模型预测的词语映射到更多的关系标签）。 (2) 输入文本中不同词的重要性不同: 在关系抽取中，输入句子中的不同词语重要性可能不同（例如，提到实体的词语更有可能与关系相关），但在分类任务中使用的原始 prompt 模板往往平等对待句子中的每个词，难以轻易反映这种差异。 自适应的答案选择方法 (adaptive answer selection method) 来解决问题 (1)，并针对问题 (2) 构建了 面向任务的 prompt 模板 (task-oriented prompt template construction)。在后者中，他们使用特殊的标记（例如 [E]）在模板中高亮实体提及的位置，以此来区分不同词语的重要性。 Semantic Parsing (语义解析) 语义解析任务是给定一个自然语言输入，生成其对应的结构化意义表示（比如将其转换为某种程序语言或逻辑形式） “Reasoning” in NLP (自然语言处理中的“推理”)Commonsense Reasoning (常识推理) 一些常见的常识推理任务包括解决 Winograd Schemas（由 Levesque 等人提出的一种句子对），这类任务要求模型识别上下文中一个歧义代词（如 it, he, she）的先行词（它指代的是谁）；或者是在给定多个选项的情况下完成一个句子。 对于前者（Winograd Schemas），一个例子是：“奖杯放不进棕色的手提箱，因为它太大了。” 模型需要推理出“it”指的是奖杯还是手提箱 通过在原句子中将“it”替换成可能的先行词（如“奖杯”和“手提箱”），并计算不同选项（替换后的句子）的概率，预训练语言模型可以通过选择概率最高的那个选项来取得相当不错的效果（引用了 Trinh 和 Le 的研究） 对于后者（给定选项完成句子），一个例子是：“埃莉诺提出给她的访客煮些咖啡。然后她意识到她没有干净的 [Z]。” 备选答案是“杯子 (cup)”、“碗 (bowl)”和“勺子 (spoon)”。 对于这类任务，我们同样可以计算每个备选词语（填充到 [Z] 位置后）的生成概率，并选择概率最高的那个作为答案（引用了 Ettinger 的研究）。这同样是利用模型预测概率的 Prompting 方式。 Mathematical Reasoning (数学推理) 在预训练语言模型的背景下，研究人员发现，当数字位数较少时，预训练的 embedding 和语言模型可以执行简单的加法和减法等运算，但当数字变大时就会失败（列出了一些相关研究） Reynolds 和 McDonell (2021) 探索了更复杂的数学推理问题（例如，如果 f(x) = x * x，那么 f(f(3)) 是多少？）。他们通过将问题的推理过程 序列化 (serializing reasoning)（一步一步地展开推理过程，比如先计算 f(3)，再计算 f(f(3))），从而提高了语言模型在这类问题上的性能。这暗示了通过将推理步骤融入到 prompt 的结构或内容中来引导模型进行链式思考。 Question Answering (问答) 问答任务有多种不同的格式，比如： 抽取式问答 (extractive QA): 答案直接从上下文文档中抽取（例如 SQuAD 数据集）。 多项选择问答 (multiple-choice QA): 模型需要从几个备选答案中选择一个（例如 RACE 数据集）。 自由生成式问答 (free-form QA): 模型可以生成任意的文本字符串作为回答（例如 NarrativeQA 数据集）。 使用语言模型（并且结合 Prompting 方法）来解决问答问题的一个好处是，可以将不同格式的 QA 任务在 同一个统一的框架 (same framework) 下解决。 但也有研究发现，直接依赖模型预测的概率来判断 QA 答案的正确性可能不太可靠。 Text Generation (文本生成)Prompting 方法可以很容易地应用于这些任务，通常是结合使用 前缀式 prompt (prefix prompts) 和 自回归预训练语言模型 (autoregressive pre-trained LMs) Schick 和 Schütze (2020) 探索了在少样本文本摘要任务中使用第 7.2.4 节描述的 固定提示词的语言模型调优 (fixed-prompt LM tuning) 方法，他们使用了手动设计的模板并微调语言模型。 Li 和 Liang (2021) 在少样本设置下的文本摘要和数据到文本生成（data-to-text generation，即将结构化数据转换为自然语言文本）任务中，研究了第 7.2.3 节描述的 固定语言模型的提示词调优 (fixed-LM prompt tuning) 方法。他们的方法是在输入前面加上一些 可学习的前缀 token (learnable prefix tokens)（即软 prompt 的一种），同时保持预训练模型的参数固定不变。 Dou 等人 (2021) 在文本摘要任务中探索了第 7.2.5 节描述的 提示词 + 语言模型调优 (prompt+LM tuning) 策略。他们使用了 可学习的前缀 prompt (learnable prefix prompts)，并通过不同类型的引导信号进行初始化，然后这些可学习的 prompt 参数可以与预训练语言模型的参数一起进行更新（联合训练）。 Automatic Evaluation of Text Generation (文本生成的自动评估)Yuan 等人 (2021b) 的研究表明，prompt learning（基于 prompt 的学习方法）可以用于对生成的文本进行 自动评估 (automated evaluation)。这意味着他们不用传统评估指标（如 BLEU），而是利用 Prompting 的方式让模型自己来评估生成文本的质量 他们通过实验发现，在使用预训练模型进行（机器翻译）评估时，仅仅在被评估的翻译文本中加入一个简单的短语“such as”（例如将原文、机器翻译结果以及“such as”一起作为 prompt 的一部分输入模型），就能在德语-英语机器翻译（MT）的评估中，显著提高评估结果与人工判断结果的 相关性 (correlation)。 Multi-modal Learning (多模态学习)Tsimpoukelli 等人 (2021) 将 prompt learning 的应用从纯文本的自然语言处理领域，转移到了 多模态设置 (multi-modal setting)，具体是结合了 视觉 (vision) 和 语言 (language)。 他们采用了前面第 7.2.3 节讨论的 固定语言模型的提示词调优策略 (fixed-LM prompt tuning strategy)，并结合了 提示词增强 (prompt augmentation) 技术。 具体来说，他们将每一张图像表示为一个 连续的 embedding 序列 (sequence of continuous embeddings)。然后，他们使用一个参数 固定（frozen） 的预训练语言模型，将这个图像 embedding 序列作为 前缀 (prefix) 输入给模型，让模型去生成文本，比如 图像描述 (image captions)。","link":"2025/05/10/NLP1/"},{"title":"TOPsis","text":"这是我的Topsis笔记 优劣解距离法（TOPSIS）层次分析法不能有太多的决策层，否则难以通过一致性检验 判断矩阵的结果不够客观，已有别的数据就暗示不要使用层次分析法 一个小例子： 如何给已有成绩的四个人评分？ 排名倒置 归一化$\\frac{x-min}{max-min}$ 归一化不合理之处： 很多指标不存在理论上的最大最小值 没有办法直接综合多个指标 极大型指标（效益性指标）：越大越好 极小型指标（成本性指标）：越小越好 统一指标类型指标正向化（把所有指标转化为极大型）极小型转化为极大型：$max-x$ 标准化（消去量纲）$$z_{ij}=\\frac{x_{ij}}{\\sqrt{\\sum_{i=1}^n x^2_{ij}}}$$ 计算得分$$\\frac{x与最小值距离}{x与最小值距离+x与最大值距离}$$ 正理想解：$Z^+=(max{z_{11},z_{21},···}，···，max{z_{1m},z_{2m,···}})$ 负理想解：$Z^-=(min{z_{11},z_{21},···}，···，min{z_{1m},z_{2m,···}})$ 第i个评价对象与正理想解的距离：$$D^+i=\\sqrt{\\sum{j=1}^m w_j(z_{ij}-S^+_j)^2}$$$D^-_i$同理 $S_i=\\frac{D^-_i}{D^-_i+D^+_i}$ 全流程原始矩阵正向化四种指标： 极大型：越大越好 极小型：越小越好 中间型：越靠近中间某值越好 区间型：落在某区间最好 正向化：所有指标转化为极大型 极小型：$max-x$ 中间型：$M=max{|x_i-x_{best}|}$,$\\tilde{x_i}=1-\\frac{|x_i-x_{best}|}{M}$ 区间型指标转换为极大型指标的数学公式如下： 设区间型指标的最优区间为 $[a, b]$，数据最小值为 $x_{\\text{min}}$，最大值为 $x_{\\text{max}}$，则转换公式为：$$y =\\begin{cases}\\displaystyle \\frac{x - x_{\\text{min}}}{a - x_{\\text{min}}}, &amp; x &lt; a, \\1, &amp; a \\leq x \\leq b, \\\\displaystyle \\frac{x_{\\text{max}} - x}{x_{\\text{max}} - b}, &amp; x &gt; b.\\end{cases}$$ 总结：极小型使用最大值减原值变成极大型；中间型先找离中心最远的距离，再化为1-（原值离中心距离/最远距离）；区间型先找离区间最远距离，再看区间外离区间距离/最远距离 标准化（消去量纲）$$z_{ij}=\\frac{x_{ij}}{\\sqrt{\\sum_{i=1}^n x^2_{ij}}}$$ 总结：每一列的每个值除以本列所有元素平方和开方 计算得分$$\\frac{x与最小值距离}{x与最小值距离+x与最大值距离}$$ 正理想解：$Z^+=(max{z_{11},z_{21},···}，···，max{z_{1m},z_{2m,···}})$ 负理想解：$Z^-=(min{z_{11},z_{21},···}，···，min{z_{1m},z_{2m,···}})$ 第i个评价对象与正理想解的距离：$$D^+i=\\sqrt{\\sum{j=1}^m w_j(z_{ij}-S^+_j)^2}$$$D^-_i$同理 $S_i=\\frac{D^-_i}{D^-_i+D^+_i}$ 总结：先找本指标的最大最小值，然后分别计算每个值与最大最小值的平方和开方作为与理想解的距离 ​","link":"2025/03/23/TOPsis/"},{"title":"pytorch&amp;DL tutorial (LiMu)","text":"fatecantkillme/-limu github库地址 预备知识数据预处理http://fatecantkillme.github.io/2025/04/24/pandas%E6%95%99%E7%A8%8B/ 线性代数标量仅包含一个数值被称为标量（scalar） 向量标量值组成的列表。这些标量值被称为向量的元素(element)或分量(component) 矩阵矩阵将向量从一阶推广到二阶。矩阵,我们通常用粗体、大写字母来表 示(例如,X、Y和Z),在代码中表示为具有两个轴的张量。 张量就像向量是标量的推广,矩阵是向量的推广一样,我们可以构建具有更多轴的数据结构。张量(本小节中的 “张量”指代数对象)是描述具有任意数量轴的n维数组的通用方法 降维1sum(axis=,keepdims=) axis:0降行的维度1降列的维度 keepdims：是否保留维度不变 点积1torch.dot(x,y) 矩阵乘法torch.mm(x,y) 范数欧几里德距离 torch.norm(u) $L_1$范数： torch.abs(x).sum() 自动微分pytorch会自动生成计算图，再根据计算图反向计算导数 基本上只需要调用loss.backward(),系统就会自动计算导数并且记录到x.grad 分离计算有时候希望将某些变量视作常数而不是其他变量计算的产物即想剪枝计算图，此时可以使用detach()产生一个新的变量与原变量拥有相同的值但是不保留计算图，这样梯度就不会向后流 其原理在于链式定理中如果把某变量单纯视作常数就不会计算它关于变量的导数了 线性神经网络线性回归线性假设是指目标(房屋价格)可以表示为特征(面积和房龄)的加权和 权重决定了每个特征对我们预测值的影响。b称为偏置(bias)、 偏移量(offset)或截距(intercept)。偏置是指当所有特征都取值为0时,预测值应该为多少 损失函数： 损失函数(loss function) 能够量化目标的实际值与预测值之间的差距 线性回归刚好是一个很简单的优化问题。与我们将在本书中所讲到的其他大部分模型不同,线性回归的解可 以用一个公式简单地表达出来,这类解叫作解析解(analytical solution) 随机梯度下降 梯度下降最简单的用法是计算损失函数(数据集中所有样本的损失均值)关于模型参数的导数(在这里也可 以称为梯度)。但实际中的执行可能会非常慢:因为在每一次更新参数之前,我们必须遍历整个数据集。因此, 我们通常会在每次需要计算更新的时候随机抽取一小批样本,这种变体叫做小批量随机梯度下降(minibatch stochastic gradient descent)。 **|B|表示每个小批量中的样本数,这也称为批量大小(batch size)。η表示学习率(learning rate)。批量 大小和学习率的值通常是手动预先指定,而不是通过模型训练得到的。**这些可以调整但不在训练过程中更新 的参数称为超参数(hyperparameter)。调参(hyperparameter tuning)是选择超参数的过程。 矢量化加速 将变量矢量化后计算速度比一个一个元素历遍计算快的多 softmax回归分类问题一般分类问题与类别之间的自然顺序无关 表示分类数据的简单方法：独热编码 softmax回归也是一个单层神经网络且是全连接层 softmax函数能够将未规范化的预测变换为非负数并且总和为1,同时让模型保持可导的性质 我们首先对每个未规范化的预测求幂,这样可以确保输出非负。为了确保最终输出的概率值总和为1, 我们再让每个求幂后的结果除以它们的总和 对数似然","link":"2025/04/25/pytorch&DL/"},{"title":"pandas tutorial","text":"Series特点： 可以容纳各种数据类型 创建后大小不变 可以包含缺失数据NaN 创建Series使用pd.Series(data=,index=,dtype=,name=,copy=,fastpath=)创建Series index参数用于指定索引（一般是一个数组） 123a=[1,2,3]myavr=pd.Series(a,index=[&quot;x&quot;,&quot;y&quot;,&quot;z&quot;])print(myvar[&quot;y&quot;]) output：2 使用字典可以直接指定索引： 12345a={&quot;x&quot;:1,&quot;y&quot;:2,&quot;z&quot;:3}myvar=pd.Series(a)print(a[&quot;x&quot;]) output:1 如果只需要字典一部分值则直接指定索引，对应索引的值会被保留 常用方法 index：获取索引 value：获取数据 head(n):获取series的前n行 tail(n):获取series的后n行 dtype：数据类型 shape：行数 describe():返回Series的统计描述的series isnull():返回布尔Series，表示是否每一个元素都是NaN notnull():表示每个元素是否不是NaN unique():去重 value_count():统计值出现的次数 map(func)\\apply(func):将指定函数应用于每一个元素 astype(dtype):将series转换为指定的数据类型 sort_value():将series根据value排序 sort_index():将series按照index排序 dropna():将缺失值NaN删除 fillna(value):填充缺失值 replace(to_replace,value):替换series中指定值 cumsum():累计求和 cumprod():累计求积 shift(period):位移 rank():返回元素排名 corr(other):计算皮尔逊相关系数 cov(other):计算协方差 to_list():将series转换为列表 to_frame():将series转换为dataframe iloc():通过位置索引 loc():按照标签索引 DataFrameDataFrame类似于二维表格 既有行索引也有列索引 DataFrame构造方法1pandas.DataFrame(data=,index=,columns=,dtype=,copy=) index:行索引 columns：列索引 数据清洗常见步骤： 缺失值处理 重复数据处理 异常值处理 数据格式转换 标准化与归一化 归一化 类别数据编码 文本处理 数据抽样 特征工程 清洗空值1dataframe.dropna(axis=,how=,thresh=,subset=,inplace=) axis:逢空去除0：行 1：列 how：’any’只要有就去除还是’all’全都是空值才去除 thresh=多少空值可以保留 subset：设置想要检查的列 inplace：是否修改源数据 在read_csv函数中可以添加na_values=[]的参数添加将什么值认定为空值 1dataframe.fillna(value=,method=,axis=,inplace=,limit=) limit:限制填充几个空值 method：’ffill’用前一个有效值填充‘bdfill’用后一个有效值填充 常用高级填充： mean():均值 median()：中位数 mode():众数 清洗错误格式数据123for x in df.index: if df.loc[x, &quot;age&quot;] &gt; 120: df.loc[x, &quot;age&quot;] = 120 123for x in df.index: if df.loc[x, &quot;age&quot;] &gt; 120: df.drop(x, inplace = True) 清洗重复数据1DataFrame.duplicated(subset=None, keep='first') 参数： 参数 说明 subset 指定判断重复的列（默认所有列） keep 保留策略： - 'first'（默认）：标记重复行，但第一个出现的不标记 - 'last'：标记重复行，但最后一个出现的不标记 - False：所有重复行均标记为True 1DataFrame.drop_duplicates(subset=None, keep='first', inplace=False, ignore_index=False) 参数： 参数 说明 subset 指定判断重复的列（默认所有列） keep 保留策略： - 'first'（默认）：保留第一个出现的行 - 'last'：保留最后一个出现的行 - False：删除所有重复行 inplace 是否原地修改数据（默认 False，返回新对象） ignore_index 是否重置索引（默认 False，保留原索引） 性能优化 大数据集：使用 subset 指定关键列减少计算量。 内存优化：inplace=True 可减少内存占用（但慎用，会直接修改原数据）。","link":"2025/04/24/pandas%E6%95%99%E7%A8%8B/"},{"title":"中国哲学简史","text":"这是我的《中国哲学简史》读书笔记 中国哲学简史中国哲学精神哲学在中国文化的地位 哲学是对人生的==系统==反思 每一种宗教就是哲学披上迷信，教义，利益，体制 哲学的功能并不是为了增进知识，而是为了提升人的心灵，==超越现实世界== 中国哲学的问题与精神中国哲学的最高追求在于成为圣人，而圣人追求与宇宙同一。表面上中国人注重社会而非宇宙，“未知生焉知死”。==中国哲学的任务就是消除入世与出世的对立。==这就是中国哲学的精神。 圣人标准：1.理论与行动统一 2.既出世又入世 3.内圣外王（有最高精神成就，且最适合为王） 中国哲学的任务：==使人有内圣外王的人格== 所以中国哲学必定与政治相关联 中国哲学家的表达方式中国哲学家习惯用名言，比喻的方式表达而不是严谨的推导，它们的暗示几乎是无穷的。 曾见郭象注庄子，识者云:却是庄子注郭象 中国哲学的背景地理背景中华文化是==大陆文化==，中国人认为中华大地即为世界，与海洋文明大不相同 经济背景传统农业国家，社会，政策围绕土地利用与分配进行。中国传统思想中的：“本”指农业，“末”值商业 ==读书与种地==是中国最光荣的两种职业 反者道之动一切事物包含他自己的否定，事物发展到了一个极端就会向另一个极端发展 自然理想化对自然的热爱和赞美被发挥到了极致 家族制度儒家学说大部分是在论证这种家族制度存在的合理性，这种家族制度是一定的==经济制度的产物==，也是地理的产物 入世与出世儒家学说是==社会组织的哲学==，也是日常哲学 我们把儒家中趋向于道家的人称为新儒家，把道家中趋向儒家的人称为新道家 这两家的并存使中国人对入世与出世有相对较好的==平衡感== 中国哲学的方法论概念分为直觉概念与假设概念 从假设概念出发的哲学家==喜欢有区别的==，从直觉概念出发的哲学家==喜欢无区别的== 在审美连续体中没有主客观的概念，所以知识论一直在中国发展不起来 海洋与大陆国家在城邦中，大家只是五湖四海来的商人，彼此之间没有从属关系。但是在家邦中天然存在等级与独裁。 农的生活方式是顺应自然的，是淳朴天真的。中国的不少发明创造都是在被阻挠。而海洋国家的人见到不同民族的人，惯于变化鼓励工艺商品创新。 中国哲学中可变与不变任何民族或时代的哲学总有一部分只想对于那个==民族==或者==经济条件==而言有价值 无论是中国的还是希腊的哲学中总是包含一些”社会一般“的东西 各家起源司马谈与六家1.阴阳家：阴阳宇宙生成论 2.儒家：传授古代典籍的教师 3.墨家：严格的一个组织 4.名家：重名实之辩 5.法家：政府建立在成文法典上而不是道德 6.道德家： 各家起源理论上古官师不分。周朝衰败后，朝中各官吏开始讲学。儒家出于司徒（掌管邦国重任外，亦有教育之责），道家出于史官········ 孔子：第一位教师孔子与六经孔子并非六经的作者，也并非是编者 在孔子之前只有官方著作没有私人著作。六经又称六艺，是周朝前期贵族的重要教育资料。流散在庶民中，以教授典籍为生，还靠当司仪为生的人叫做”儒“。 孔子作为教育家孔子在传述传统制度与观念时是由他自己的道德观念推导出来的。比如对“三年治丧”的解释。所以孔子在述的同时作了一些东西。 正名正名：名实相符 拥有特定的社会身份就要承担相应的责任 仁义人有本身就应该做的事情 人在社会中的义务的本质就是爱人，仁==爱人。由于在社会中只要爱人就能产生所有的德性，故仁人就是全德之人 忠恕“己所之欲，亦施于人”称为“忠”，“己所不欲勿施于人”称为“恕”。这是以自身尺度调节自身行为的正反两面。行忠恕之道就是行仁，这是一种爱人的行为。“夫子之道，忠恕而已。”表明了忠恕是行仁的基本方法。“仁远乎哉？我欲仁，斯仁至矣。” 知命“知其不可为而为之”这是对于外部世界随机性的无视。这是一种行为自洽，当我们不再患得患失，我们也就成为无敌的超人了。“智者不惑，仁者不忧，勇者无惧”。因为仁者做事只是因为这样符合自己的行事准则，而不是为了某种目的去做，所以目的是否达成也就不是那么重要了，所以知命的仁者不会忧虑 孔子的精神修养发展过程孔子一直到四二而不惑则认识到了所有的道德价值，但此时认识到的道德价值并没有完全融入日常行为中。所以才会有七十而从心所欲不逾矩。在四十岁过后孔子开始认识到了超越道德的价值。即“天命”，但是孔子认识到的超越道德的价值并没有让他进入混沌的，神秘的道家思想中，反而他开始相信自己的行为是上天所指派。 孔子的历史地位孔子在生前不过是一位普通的教师，但他确确实实是一位十分博学的人，再加上他的主要工作是以述为作。将贵族教育中的中华文明的瑰宝传授给了很多人，在那个英才辈出，纷争不断的时代无疑受到他教育的人会站在社会的高层，故而孔子建立起了话语权。在其死后，在公元前二世纪，他被认为是无冕之王。在汉朝由于独尊儒术的影响，加上汉朝统治者为了加固自己统治合法性传出来的孔子预言了汉朝的谣言，导致了孔子被神话的现象。 墨子：孔子的第一个反对者孔子是传统文化的辩护者，而墨子是一位革命者，他力求把古代落后的礼乐制度用他认为更实用，更“仁爱”的方法取代 墨家的社会背景古代的武士往往是世袭的。在封建制瓦解后，这些武士变成了雇佣兵，称为’侠’。而墨子建立了一个军事组织并充分论证了‘侠义’精神的正确性，这就是墨家。他们不是传统的武士群体，他们是为了‘侠义’而战斗的理想主义者。且这个群体代表的是庶人的观点。 墨子对儒家的批评就是儒家不敬鬼，还铺张浪费，还信天命","link":"2025/03/22/%E4%B8%AD%E5%9B%BD%E5%93%B2%E5%AD%A6%E7%AE%80%E5%8F%B2/"},{"title":"倦怠社会","text":"本文是《倦怠社会》的读书笔记 超越规训社会 在过去的社会中，否定性的规训充斥着社会，“不允许”和“应当”带有的否定性和强制性被现在的功绩型的一个更加积极打破了否定性的词“能够”所取代，把原来的道德，法律，禁令以项目计划，自发行动，内在动机所取代。 这种转变产生于生产最大化的集体潜意识，功绩主体较规训主体更有效率，但是规训主体仍接受规训只是他自己逾越了规训 在晚期现代社会的绩效命令造成了抑郁症 现代大量出现的不是尼采的独立自主的超人，而是没有了主权的末人，在没有任何外部压迫情况下会自愿进行自我剥削 由于进行剥削的是自己，因此这突破了心理免疫系统，当功绩主体不能完成绩效时抑郁症便在这一刻爆发，这导向了毁灭式的自我谴责和自我攻击 绩效社会之所以能更高效地剥削是由于它附带了一种自由的感觉 这一章节可以类比于以前的规训社会是生产了一群没有预设思想的机器人，在任他们发展的道路上设立了很多路障以规训人们，现在是在机器人的脑中事先烧录进一些固有程序然后几乎不设限制的让其自由发挥。在以往，规训的道路毕竟不会太窄，大家只要在规训范围内就行，现在大家都一定想准确的走由自己脑中的程序所决定的道路，一旦偏离就会出现问题，很明显后者的效率远超前者，且对于机器人来说这似乎是自由的，没有人强迫自己，所以会毁灭式的责怪自己 深度无聊 由于过度的信息刺激和现代工作的特殊需求，我们的注意力结构发生了根本的改变，变得分散，碎片化 由于我们已经习惯了在很多的任务之间切换，我们已经无法忍受深度无聊，我们必须把我们分散的注意力不断地在多个工作之间切换 再去除了冥想悠闲后人会表现出一种超积极性，永不安宁的人会大行其道，我们的文明会重新陷入一种野蛮的状态 倾听的能力以沉思的注意力为基础，过度积极的个体无法获得倾听的能力 然而具有忍受深度无聊的个体所创造出来的不一定符合绩效原则，因为绩效不再是他们眼中的唯一 我们涣散的注意力对于现代社会来说确实是必不可少的，但是对于深度无聊的忍受其实任然是可以并存的，大家都说人生不过那几个瞬间，但是我们真的必须要去为了特定的几个瞬间而放进我们所有的时间吗，什么叫浪费时间？时间真的可以被浪费吗？也许现代的深度无聊是过度的刷抖音，过度的玩游戏，这都会造成无聊，但是走在路上或者坐在公交车上我倒不觉得无聊，任由思绪漂浮还是感觉很好，但是在某一个固定的地方坐着似乎就会比较无聊了，思绪也飞不起来了","link":"2025/03/22/%E5%80%A6%E6%80%A0%E7%A4%BE%E4%BC%9A/"},{"title":"从零到壹","text":"这是我的《从零到壹》笔记 进步的未来进步分为： 全面进步即全球化即复刻发达国家的发展路径 垂直进步即科技发展，即发明新方法提高生产效率 全球化并不能解决所有问题，因为资源有限，如果垂直进步跟不上，盲目的全面进步只会导致灭亡 创业思维所谓创业，就是一群人对当下产业环境不满，希望通过自己的努力让世界更美好。对于一个初创公司来说最重要的不是效率不是灵活性，而是抛弃原有观念，重零审视自己从事的业务 像1999年那样狂欢反主流事实：个人不容易发生精神错乱，但是群体，政党，国家却很容易发生精神错乱，如果能看透这些不切实际的大众观点，你就能看到背后的反主流事实 20世纪90年代互联网热潮由于对科技的过分信心导致了市场的虚假繁荣，在千禧年到来之初发生了纳斯达克指数的崩溃，宣告了这个科技乐观主义的时代的结束，之后全球化代替科技成为新一轮泡沫 经验教训： 心存改变世界的雄心的人往往更加谦逊小心，小幅度的，循序渐进的增长是安全前进的唯一通道 保持精简，灵活 多留出空间 在改进中竞争，改进被认可的产品，在次级市场保持垄断地位 专注产品，而非营销 所有成功的企业都是不同的经济学中的两种现象 完全竞争：处于竞争市场中的每一家公司都没有差别，卖的产品同质化，价格完全由市场决定，当供少于需则利润上涨，新公司产生，此时供给量上升利润下降，就不会有新公司，如果新公司过多则公司就会倒闭，最终价格又回到稳定水平 垄断：拥有自己的市场，可以自由定价以实现利益最大化 企业的谎言垄断者和竞争者都会歪曲事实以维护自己的利益 垄断者的谎言：通过将自身企业面向的市场定性为各种大市场的并集避开那些自己实际形成了垄断地位的业务从而避免审查，打击 非垄断者则会标榜自己的目标市场是各种小市场的交集以展现其垄断性。 我们一定要注意可能这样的市场原本不存在的原因可能是不存在这些市场的消费者，同时也不能忽略原本存在的激烈竞争而只是专注于自己那微小的差异化，因为归根到底你的竞争可能仍在这些小市场的并集中产生 市场的无情在激烈的竞争下，基本的人文关怀是不能做到的。只有谷歌那样的垄断性企业才能更多的关心自己的员工，产品，和更广阔的影响力。在完全竞争中，企业只能着眼于眼前的短期利益不可能进行长期规划，解决这些问题的唯一办法就是：获得垄断性利润","link":"2025/03/23/%E4%BB%8E%E9%9B%B6%E5%88%B0%E5%A3%B9/"},{"title":"关于我最喜欢的B站频道要更第二季了这件事","text":"床哥终于也是要重出江湖了 😊 犹记在一个月黑风高的夜晚​ 走读的我睡前惯例点开B站时，命运的大手让我刷到了床哥。起初以为是徐云式的骑行博主，但床哥明显太腼腆了，走哪里都不忘反侦察，视频没有明确的策划，而是一五一十地把一整天都拍了下来。这种无聊的土味视频是不能吸引我的，但是弹幕刷着各种梗，我点开了床哥的主页。我完全被镇住了，老哥原来不是偶尔找不到旅馆才睡桥洞，单纯是桥洞收集爱好者，各种铁皮桥洞，单开间双开间，雪地桥洞、沙漠桥洞、森林桥洞、更有油炒水的狠活（他怎么做到既没有生存技能还能野外求生的？🤔）。在恨不得紫砂来吸引眼球的互联网中，床哥用无意识的生存本能完成了最硬核的行为艺术。那些在工业糖精式搞笑里逐渐麻木的神经，突然被原始粗粝的真实感激活，像尝到了旷野里带沙砾的野莓。（当时我已经开始逐渐开始厌倦小潮团队的强行整活了😥） 毫无技法的纪录片​ 在人人举着4K云台的流量丛林里，床哥带着一部小米手机，把自己活成了一部行走的《路边野餐》。全网都在用工业级滤镜腌制生活时，他笨拙的骑行运镜里晃动着老式DV的噪点美学——BGM总是在情绪制高点突然断电，长镜头总爱对着生锈的公路护栏发呆，连吃泡面的特写都要被呼啸而过的货车声碾碎。这些违反算法逻辑的”事故”，恰恰构成了赛博荒野里最珍贵的故障艺术。他面对镜头时的局促比任何网红笑容都鲜活，遇到陡坡时孩子气的欢呼比百万级运镜更动人。当骑行博主们忙着在318国道上演《荒野求生》真人秀时，床哥的镜头却总在捕捉牧羊少年皲裂的脸庞、货运驿站里结着茶垢的保温杯。那些未经修辞的生存褶皱里，藏着算法永远无法解码的人间质地。最致命的是他的”去表演性”生存哲学：不会对着星空朗诵仓央嘉措的诗，不会给破轮胎加悲情滤镜，甚至记不住自己睡过的第127座桥洞编号。这种近乎钝感的真实，反而让他在精心编排的人设狂欢中，成为了流量荒漠里最后一块绿洲。当我在算法投喂的楚门世界里日渐麻木时，突然撞见这个不会说”老铁666”的像素野人，竟像摸到了互联网消失已久的体温。 一个词穷的演讲者​ 每个视频几乎都是以床找到了桥洞搭起帐篷，煮了饭后边吃饭边对着镜头说话。他絮絮叨叨重复着自己说过的话，似乎那些话不是说给镜头前的观众。他的词不多，转着车轱辘话被网友戏称为床言床语。但是当他走入崇高的自然场景中，在一个又一个“世界名画”中，一切都透过了符号界直接闯入实在界，野草的香通过狂风抵达了我。一位词穷演讲者此刻诉说了所有他要说的。","link":"2025/03/23/%E5%85%B3%E4%BA%8E%E6%88%91%E6%9C%80%E5%96%9C%E6%AC%A2%E7%9A%84B%E7%AB%99%E9%A2%91%E9%81%93%E8%A6%81%E6%9B%B4%E7%AC%AC%E4%BA%8C%E5%AD%A3%E4%BA%86/"},{"title":"层次分析法","text":"这是我的层次分析法笔记 评价类模型层次分析法（AHP） 评价类问题可以用打分解决（注意评价时各指标权重和为1，同指标打分和也为1） 权重指标 方案一 方案二 ······ 指标一 指标二 指标三 ······ 评价类问题的三个问题： 目标？ 方案？ 指标？（文献、常识法、快搜、谷歌搜索） 如何确定权重？ 两两指标比较推出权重 标度 含义 1 两因素同等重要 3 一个比另外一个稍微重要 5 明显重要 7 强烈重要 9 极端重要 2，4，6，8 上述判断的中值 倒数 A和B比是3，那么B和A比就是1/3 指标一 指标二 指标三 指标四 指标一 $a_{ij}$ 指标二 指标三 指标四 判断矩阵的内容用专家系统利用上表填 $a_{ij}$的意义：与j相比i的重要程度 当i==j，为1 $a_{ij} \\times a_{ji}==1$(正负反矩阵) 怎么在一个指标上给方案打分? 依旧用判断矩阵 指标 方案1 方案2 ······ 方案1 方案2 ······ 可能会出现“不一致”现象 $a_{ij}=\\frac{i的重要程度}{j的重要程度}$ $a_{jk}=\\frac{j的重要程度}{k的重要程度}$ $a_{ik}=\\frac{i的重要程度}{k的重要程度}=a_{ij}\\times a_{jk}$ 一致矩阵特点：各行（各列）之间成倍数关系 **注意：**使用判断矩阵求权重之前，一定要进行一致性检验,即$a_{ik}=\\frac{i的重要程度}{k的重要程度}=a_{ij}\\times a_{jk}$ 一致性检验 充要条件： $a_{ij}&gt;0$ $a_{ij}=1 \\quad \\text{if } i==j$ $[a_{i1},a_{i2},···,a_{in}] =k_i[a_{11},a_{12},···,a_{1n}]$ 一致矩阵有一个特征值为n，其余特征值为0 越是不一致，最大特征值与n相差就越大 一致性检验标准步骤 计算一致性指标CI $$CI=\\frac{\\lambda_{max}-n}{n-1}$$ $\\lambda_{max}$就是最大特征值 查找对应平均随机一致性指标RI 计算一致性比例CR $$CR=\\frac{CI}{RI}$$ 若CR&lt;0.1则可以接受 一致矩阵怎么计算权重1. 构建判断矩阵首先，构建一个 $ n \\times n $ 的判断矩阵 A ，其中元素 $ a_{ij} $ 表示因素 i 相对于因素 j 的重要性。 示例假设有 3 个因素 $C_1, C_2, C_3 $，判断矩阵如下：$$A = \\begin{bmatrix}1 &amp; 3 &amp; 5 \\\\frac{1}{3} &amp; 1 &amp; 2 \\\\frac{1}{5} &amp; \\frac{1}{2} &amp; 1\\end{bmatrix}$$ 2. 计算权重向量特征法通过求解判断矩阵的特征向量来计算权重。以下是具体步骤： 步骤 1：计算矩阵 ( A ) 的最大特征值 $\\lambda_{\\text{max}} $最大特征值 $\\lambda_{\\text{max}} $ 是矩阵 ( A ) 的最大特征值。 步骤 2：计算特征向量 ( w )特征向量 ( w ) 是对应于 $\\lambda_{\\text{max}} $ 的特征向量，归一化后即为权重向量。 步骤 3：归一化特征向量将特征向量 ( w ) 归一化，得到权重向量。 3. 具体计算过程以下是具体的计算方法： 步骤 1：计算矩阵 ( A ) 的特征值和特征向量使用数学工具（如 MATLAB、Python 等）计算矩阵 ( A ) 的特征值和特征向量。 步骤 2：选择最大特征值 $\\lambda_{\\text{max}} $从所有特征值中选择最大的一个，记为 $\\lambda_{\\text{max}} $。 步骤 3：提取对应特征向量 ( w )选择与 $\\lambda_{\\text{max}} $ 对应的特征向量 ( w )。 步骤 4：归一化特征向量将特征向量 ( w ) 归一化，得到权重向量：$$w_i = \\frac{w_i}{\\sum_{j=1}^{n} w_j}$$以下是对上述矩阵 ( A ) 的计算过程： 步骤 1：计算特征值和特征向量假设计算得到特征值和特征向量如下：• 特征值：$ \\lambda_1 = 3.039, \\lambda_2 = 0.019, \\lambda_3 = -0.058 $• 特征向量：$$w = \\begin{bmatrix}0.633 \\0.267 \\0.100\\end{bmatrix}$$ 步骤 2：选择最大特征值最大特征值为 $\\lambda_{\\text{max}} = 3.039 $。 步骤 3：提取对应特征向量特征向量为：$$w = \\begin{bmatrix}0.633 \\0.267 \\0.100\\end{bmatrix}$$ 步骤 4：归一化特征向量归一化后得到权重向量：$$w = \\begin{bmatrix}0.633 \\0.267 \\0.100\\end{bmatrix}$$ 层次分析法（Analytic Hierarchy Process, AHP）是一种将复杂决策问题分解为层次结构，通过量化主观判断进行多准则决策的方法。以下是其大体步骤： 1. 建立层次结构模型• 目标层：明确最终目标（例如“选择最佳供应商”）。• 准则层：分解影响目标的关键因素（如价格、质量、服务等）。• 方案层：列出可选方案（如供应商A、B、C）。 示例： 123目标层：选择最佳供应商 准则层：价格、质量、交货周期 方案层：供应商A、B、C 2. 构造判断矩阵• 两两比较：对同一层次的准则或方案进行重要性比较，使用 1-9标度法（1=同等重要，9=极端重要）。• 构建矩阵：例如，比较准则层中的价格、质量、交货周期，形成如下矩阵： 1234 价格 质量 交货周期价格 1 3 5质量 1/3 1 2交货 1/5 1/2 1 3. 一致性检验• 计算权重：通过特征向量法或几何平均法，得出各准则的权重。• 检验逻辑：计算一致性比率（CR）。若 CR &lt; 0.1，判断矩阵合理；否则需调整比较值。 • 公式：( CR = \\frac{CI}{RI} )，其中 ( CI = \\frac{\\lambda_{\\text{max}} - n}{n-1} )，( RI ) 为随机一致性指标（查表可得）。 4. 计算方案层权重• 对每个准则（如价格、质量），分别构造方案层的判断矩阵，重复步骤2-3，计算各方案在该准则下的权重。 5. 总排序与决策• 综合权重：将准则层权重与方案层权重相乘，得到各方案的总得分。• 排序：按总得分从高到低选择最优方案。 关键特点• 量化主观判断：通过1-9标度将定性问题转化为定量分析。• 动态调整：若一致性检验未通过，需重新调整判断矩阵。• 适用范围：适用于资源分配、风险评估、供应商选择等复杂决策问题。 示例结果：若供应商A在总排序中得分最高，则选择供应商A。通过AHP将复杂决策分解为可操作的步骤，降低主观随意性。","link":"2025/03/22/%E5%B1%82%E6%AC%A1%E5%88%86%E6%9E%90%E6%B3%95/"},{"title":"拉康式主体","text":"这是我的《拉康式主体》读书笔记","link":"2025/04/05/%E6%8B%89%E5%BA%B7%E5%BC%8F%E4%B8%BB%E4%BD%93/"},{"title":"拉康思想的基本概念及理论模型","text":"主体/主语 的考古学问题重重的现代主体在笛卡尔著名的：“我思故我在”之后，主体（subjet）被确认为思想的施动者（agent），这个主体就是所谓的现代主体 人们认为人的主体是一个理性、具有能动性的存在，是统一的，实在的 但是尼采指出：这一切不过是语言习惯导致的误认；思想不是由于存在一个主体然后才发生的，就像是闪电不一定是有雷公电母才产生的一样。思想就如同自然中一切的自然现象一般，只是由于各种力量混合造成的后果 Subjet的含义及演变在古希腊时代，sujet本身是一种相对被动的存在，他作为偶性、谓语的载体而存在，而不是作为行动的主体而存在 思想的“主体-施动者”","link":"2025/05/20/%E6%8B%89%E5%BA%B7%E6%80%9D%E6%83%B3%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%8F%8A%E7%90%86%E8%AE%BA%E6%A8%A1%E5%9E%8B/"},{"title":"精神分析引论","text":"这是弗洛伊德的《精神分析引论》笔记 精神分析引论一 ：失误行为导论在精神分析中的对话绝不允许第三者存在 心理过程主要是潜意识的，意识在心理中只占据一小部分 心理是感情·思想·和欲望的共同产物 性冲动在精神疾病形成过程中具有举足轻重的作用 同时性冲动还在人类文化 艺术 和社会创造性活动中具有举足轻重的作用 文化是在生存的压力下以牺牲欲望创造出来的，被牺牲的欲望中性冲动占据了很大一部分，他们被转移升华到了更加高尚的目的上，但是这一过程是极其不稳定的 失误行为Ⅰ失误行为用注意力不集中是不能完全解释的 所有的失误或者口误都意义非凡 失误行为Ⅱ有些失误行为本身就具有一些意义，在心理学中意义指的就是行为的目的 生理原因不算是失误行为的必要条件，但是会给失误带来便利 口误很多时候会把自己的真心话说出来 如果病人直接承认了自己口误中所带有的真实意图，那么可以直接相信他，否则就得间接找到证据 一直记不住某个人的名字可能代表本来就不想记住他 忘记原有的意图多半是对原有意图具有成见 我们不会相信自己找不到某些东西其实是潜意识中故意找不到的 遗忘往往暗示着我们潜意识中的某种想法，用别人的失误行为去揣测别人当然是不当的，但是用于窥探自己的潜意识也许是一个好方法 主观行为很多时候会把自己伪装成为一个客观行为，你很难判断一个征兆到底是主观上造成的还是纯属意外，因为我们自己并不能看到心理中的所有 失误行为Ⅲ所有的失误行为不过是潜意识意图和意识意图相互干扰的结果 干扰意图既可能与受干扰意图具有一定的关联性，也可能来自于不久前的思绪 心理学不仅要把各种的现象描述分类，还要解释其内部结构动力 存在两种相互冲突的倾向，某种倾向被压制，故而以制造口误的方式来获得补偿 遗忘的干扰因素只有一个就是不愿意做某事 人类的京精神生活其实是不同的倾向互相交战的战场，我们没有一个确切的完全的取向 丢失物品可能是物主想命运献祭以避免更大的损失的结果","link":"2025/03/23/%E7%B2%BE%E7%A5%9E%E5%88%86%E6%9E%90%E5%BC%95%E8%AE%BA/"},{"title":"精神现象学","text":"这是我的《精神现象学》笔记 导言：黑格尔把《精神现象学》作为他的科学体系第一部，但是在《哲学百科全书》只精神现象学只作为了第三章的一个小节。 邓晓芒：黑格尔是为了掩盖自己所有哲学的起源，后期在哲学百科全书做了更加具有逻辑的理解 精神现象学是一种意识的经验科学，不是逻辑性的 黑格尔认为经验性的是世俗的是堕落的，但是从逻辑学的角度理解世俗生活就拯救了世俗生活 黑格尔认为逻辑就是上帝 黑格尔尝试阐述的是逻辑本身，就是上帝本身 黑格尔认为逻辑学就是上帝创造万物的蓝图 这就是为什么黑格尔避免把精神现象学摆在哲学百科全书的最前面，因为上帝具有第一性 所以黑格尔的哲学实际上还是起源于《精神现象学》 马克思：黑格尔把自己的精神讲成世界结构，谈的始终是世界意识，即其本身的意识 精神现象学的读法： 考察的是一般意识的发展过程 将所有的偶然性避开，讨论的是意识发展结构 黑格尔：历史是逻辑的必然 超越心理学：考察的并不是人类心理，而是任何可能具有意识的普遍发展 黑格尔：现象本质分立；胡塞尔：本质即现象；康德：现象与本质分离，且本质不可知 胡塞尔：超越自然主义 心理学是科学，是自然哲学 人类学、民俗学使用实证方法也是一种科学 现象学谈的是意识本身结构 精神现象学是人类学和心理学之间的学问，介于客观性与完全主观性之间，是主观精神的客观逻辑结构 读精神现象学要做到反思、阅读自己，使用黑格尔的方法分析自己的精神结构 读精神现象学就像读小说，把别人的人生纳入到了自己的人生 要超越一个人必须了解其思想才能超越 人的精神生活的逻辑决定了历史的逻辑 精神现象学结构 可以分为五个大部分：意识（知觉）、自我意识、理性、客观精神、绝对精神（宗教、绝对知识） 前三个是主观精神，后面两个是客观精神 前三项作为主要的精神划分阶段（A、B、C） C可以分为C（AA）理性本身 C（BB）作为精神的理性C（CC）作为宗教的理性 C（DD）作为绝对知识的理性 序论：论科学认知当代的科学任务导论在学习真理本身之前，先对认知本身探究是很有必要的，因为认知是有界限的。即使我们想要摆脱认知局限对我们的影响，但是抽出认知后我们就什么都不知道了。但是关于认知具有局限性本身也具有一些假设，就是我们认识到的不是绝对真理。说白了把认知看作是受折射的光，但是这点本身也值得受到审查。黑格尔认为主客观之分是一种欺骗。 要想声明科学高于其他的异见靠的不是断言，也不是在一个全新的领域展现对方的无知，而是引导对方回到自己的观点中找到缺陷并给出一种更好的想法","link":"2025/03/23/%E7%B2%BE%E7%A5%9E%E7%8E%B0%E8%B1%A1%E5%AD%A6/"},{"title":"自伤自恋的精神分析","text":"这是我的《自伤自恋的精神分析》笔记 自我憎恶到极致的人往往也是极度以自我为中心的不顾他人的人 新自由主义：责任在你自己，你痛苦是因为你自己不行 经常自我否定的人往往不接受他人的反驳，安慰和鼓励，但是他们的自责来自于我本该非常重要非常有价值 激怒一个人最快速的两个途径：1.质疑他的审美 2.质疑做一件事情的动机 但是自己调侃这两条会显得自己很大气 所谓污名化就是社会强加给个人的不良标签会引起社会歧视，人主动的内化污名化会为自己的存在感到羞耻，人就会主动预测自己进入社会中后会陷入困境，从而丧失进入社会的意愿 谦虚的前提是有足够强大的自信 社会地位和世俗的成功并不能给人带来自信 之所以时时刻刻都自我否定不是因为不够自爱，反而这种人是很自恋的，因为时刻都无法停止把自己跟别人作比较他们始终把自己放在思考的正中心 认为自已一无是处的人其实非常痛苦无援，因为即使他人伸出援手自已也无法伸出手接受帮助，他们被周围人视为麻烦，难缠的异物 人对于与自己完全不同的人很难产生敌意，但是对于与自己只有微小差异的人就很容易，这也是由于自恋 在健全自恋的过程中最重要的就是建立像感知自己身体一般感知别人的主体-客体关系，儿童在各种主体-客体关系中会学习在关系与归属感中汲取能量，通过吸收他人的特质人格变得更加复杂更加稳定 最初的抱负心来自于母亲无条件的肯定 自立就是为自己创造更多可以依赖的对象 自信就是无条件积极认同当下的自己 自尊是对“我应该是什么样的人”的执念 有自信的人不会太执着于自尊，高自尊的人普遍没有自信，，但是当自尊与自信都崩塌时，自恋也在崩塌 在自我否定和自残的过程中其实是在下意识寻找与外界产生“关系” 同时自我否定也是表明自己比其他任何人都更了解自己的无能，表现的是不愿意把评价自己的权力让渡给他人，这也是一种自我保护 自我认同感在上升到万能感之前就可能会因为受到挫折而被制止，但是无力感由于无法被修正","link":"2025/03/23/%E8%87%AA%E4%BC%A4%E8%87%AA%E6%81%8B%E7%9A%84%E7%B2%BE%E7%A5%9E%E5%88%86%E6%9E%90/"},{"title":"荣格心理学入门","text":"这是我的《荣格心理学入门》笔记 荣格心理学入门卡尔·古斯塔夫·荣格人格结构人格结构问题 人格动力问题 人格发展问题 精神人格作为整体就是精神 精神包括了思想情感行为，包括有意识和无意识的。 精神的三个层次：意识，个人意识，集体意识 意识心理四种功能：思维，情感，感觉，直觉 两种心态：外倾：意识定向于外部客观世界 内倾：意识定向于内部世界的 自我自我就是自觉的意识，也就是我们通常意识到的“心理”。 做我作为心理门户：不被自我承认的观念，记忆，直觉不被自我承认就不会进入意识 自我保证了人格的连续性和同一性，通过对心理材料的筛选和淘汰。 一个高度个性化的自我将允许更多的东西成为意识 个人无意识所有那些微弱的，达不到意识的体验统统存留在人的个人无意识中，个人无意识中的东西更易于被意识所接受。 个人无意识对于梦的产生具有重要作用 情结一组一组的心里内容聚合在一起形成心理从称之为“情结” ==不是人在支配着情节，是情结在支配着人== 集体无意识个人无意识由意识中被遗忘的部分组成，而集体无意识在人的一生中都未背意识到。 集体无意识储存着所有的==原始意象== 集体无意识的内容被称为原型原始意象只有当它成为意识并被意识经验充满了的时候才能被确定； 原型作为中心，把与他相关的经验吸引到一起形成情结； 如果原型形成的情结在一个人的意识中占据了优势，那么这个人的所有经验都要被这种情结所统治。 每个人的人格中都具有的原型人格面具也被称为顺从原型，其作用是保证人能够扮演某种性格以被社会接受。这是社会生活的基础，一个人想要在社会上立足，就必须满足社会对它的期待。勤勤恳恳，任劳任怨都只不过是人格面具中的一部分。人格面具是精神的外部形象。 当人格面具过度膨胀以至于人的自我认同就等同于人格面具时，一方面人会因为成功充当了某种角色而骄傲自大，且常常企图把这种角色强加给别人。且自己也是受害者，当达不到自己的预期时，他会受自卑感和罪恶感的折磨。 阿尼玛和阿尼姆斯阿尼玛是男性心中女性的一面。 阿尼姆斯是女性心中男性的一面。 这两种异性特征保证了两性在相处时能保持协调和理解 人格动力精神新的人生经历会强行进入我们的精神世界并且打破精神原有的平衡，所以==人应该周期性的退回到我们的精神世界以恢复其平衡==。同时，人对于新鲜刺激有一种天然的欲望，所以正常的精神应该是介于完全开放和完全封闭之间的 心理能心里能即欲望，努力，意愿。来源于我们曾经的经历，体验。精神活动并不等同于意识活动。 心理值即分配给某一心理要素的心理能的计量。当某种很高的心理值被赋予了某种心理要素诸如：美，快乐，竞争 这会左右一个人的行为。通过对梦的记录，一个人可以测得其心理值大头。测定情结的聚合力可以测定下意识中的心理值，测定方法包括：1.直接观察和分析推理：寻找旁证，懂得绕开那些表面冠冕堂皇的话寻找情结本身。2.情节表征：任何的反常行为都表征了某种情结，叫错名字，对熟悉事物忘记，过分的情绪反应，或者对于某些词语的迟缓，出现过度补偿会使得发现某种隐藏的情结困难，比如过度锻炼自己的肌肉，对女性气质过分指责都可能是为了掩盖自卑情结。3.情绪反应4.直觉 等值原则当某一心里要素的心理能消退时，则另一心理要素的心里能就会增加 均衡原则心里能倾向于由弱的那一方流向强的那一方，外来的新的能量不断打破着这个平衡创造出不平衡，我们同他人的冲突常常是我们自己内心人格的冲突，==一个人气势汹汹地讨伐不义之时，也是在讨伐他自己内心的阴影==。当外来的情绪过于强大而难以驾驭时，我们会建立起一层坚硬的外壳以保护自己，当突破这层外壳时，情绪的发泄往往是爆发式的。但是保守的心理状态并不是好的，具有极强的偏见并且倾向于保守，但是外界不可能让你真正的保持精神的绝对平静。所谓年轻人倾向于暴躁而老年人倾向于宁静的原因大部分是心理中的心理值由于熵原理趋于平静。整个精神所具有的能量让新的外界带来的能量显得不过是沧海一粟。过分片面发展的人格是不稳定的，极容易会突然转到它的对立面去。 前行与退行前行就是人的心理适应能力发展的那些日常经验，但是由于我们的心理具有偏向性，在需要另外的心理能力时，需要进行退行，才能长足的发展其他的心理能力。 能量的疏导人的自然能量来源于人的本能，本能的能量同自然中的任何一种能量一样，会自然的流向某种方向。当把本能的能量疏导到本能的类似物中去则精神能量就起作用了，这就是象征。比如，水电站就是瀑布的象征。 人格的发展","link":"2025/03/23/%E8%8D%A3%E6%A0%BC%E5%BF%83%E7%90%86%E5%AD%A6%E5%85%A5%E9%97%A8/"},{"title":"随想25&#x2F;3&#x2F;25","text":"漫游与行为干预-Francis Alÿs 羊群被带领开始绕柱子的行为在直观上反应了例如盲从、新自由主义极权等的讽刺。但在更深层面，这似乎也暗喻了所谓的客观精神。不同之处在于，这个“圈”（可以理解为语境）并非静态，不会自动解体，而是在逃离者的带领下进入下一个“圈”，等待新的逃离者。每个人都身处其中，并隐约感觉到被某种力量引导。 通俗上讲其实这个圈就是语境。具体到当代中文政治语境，不完全的言论自由甚至言论不自由已成为一种政治正确。这种语境下，受到制约的人们 paradoxically 认可了这种现状。他们明明可以在清晰的场景下直接表达言论不自由的观点，却倾向于使用隐喻。这导致了能指链的断裂：他们通过语言暗示言论不自由是由政府造成的，但同时又回避直接言说。这种现象本身就是一种症状。从制定政策的掌权者角度来看，其首要考虑往往是政治经验。而政治经验的来源，很大程度上是顺从集体潜意识，以避免自身受到反对。即使艺术家将掌权者比作拉着羊的人，但掌权者本身也是这个“圈”中的一员，同样会受到集体意识的引导。最终，无论是谁，都身处这个不断循环的语境之中。 关于歧视 艺术的本质决定了它不可能保持中立。当你通过特定的元素限定了某种所指时，就必然排除了其他的可能性，这种排斥本身就构成了一种“歧视”。具体到张晓刚的作品，当他不断地将方框眼镜和那张面无表情的脸与作品中的讽刺意味联系起来时，这种解读上的“歧视”就已经产生（即使他试图通过使用自身形象来避免）。一旦这些面部特征的能指被固定到画家想要表达的意识上，这种“歧视”便会更加固化（从这个角度而言，一切的讽刺画作都难以避免）。","link":"2025/03/25/%E9%9A%8F%E6%83%B325_3_25/"},{"title":"ok","text":"","link":"2025/06/28/ok/"}],"tags":[{"name":"prompt engine","slug":"prompt-engine","link":"tags/prompt-engine/"},{"name":"NLP","slug":"NLP","link":"tags/NLP/"},{"name":"数学建模","slug":"数学建模","link":"tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"},{"name":"评价方法","slug":"评价方法","link":"tags/%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95/"},{"name":"python","slug":"python","link":"tags/python/"},{"name":"pytorch","slug":"pytorch","link":"tags/pytorch/"},{"name":"DL","slug":"DL","link":"tags/DL/"},{"name":"ML","slug":"ML","link":"tags/ML/"},{"name":"数据处理","slug":"数据处理","link":"tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"},{"name":"NLP入门","slug":"NLP入门","link":"tags/NLP%E5%85%A5%E9%97%A8/"},{"name":"数模","slug":"数模","link":"tags/%E6%95%B0%E6%A8%A1/"},{"name":"哲学","slug":"哲学","link":"tags/%E5%93%B2%E5%AD%A6/"},{"name":"社会学","slug":"社会学","link":"tags/%E7%A4%BE%E4%BC%9A%E5%AD%A6/"},{"name":"经济","slug":"经济","link":"tags/%E7%BB%8F%E6%B5%8E/"},{"name":"畅销书","slug":"畅销书","link":"tags/%E7%95%85%E9%94%80%E4%B9%A6/"},{"name":"网络杂谈","slug":"网络杂谈","link":"tags/%E7%BD%91%E7%BB%9C%E6%9D%82%E8%B0%88/"},{"name":"拉康","slug":"拉康","link":"tags/%E6%8B%89%E5%BA%B7/"},{"name":"后结构主义","slug":"后结构主义","link":"tags/%E5%90%8E%E7%BB%93%E6%9E%84%E4%B8%BB%E4%B9%89/"},{"name":"精神分析","slug":"精神分析","link":"tags/%E7%B2%BE%E7%A5%9E%E5%88%86%E6%9E%90/"},{"name":"法兰克福学派","slug":"法兰克福学派","link":"tags/%E6%B3%95%E5%85%B0%E5%85%8B%E7%A6%8F%E5%AD%A6%E6%B4%BE/"},{"name":"心理学","slug":"心理学","link":"tags/%E5%BF%83%E7%90%86%E5%AD%A6/"},{"name":"弗洛伊德","slug":"弗洛伊德","link":"tags/%E5%BC%97%E6%B4%9B%E4%BC%8A%E5%BE%B7/"},{"name":"黑格尔","slug":"黑格尔","link":"tags/%E9%BB%91%E6%A0%BC%E5%B0%94/"},{"name":"绝对唯心主义","slug":"绝对唯心主义","link":"tags/%E7%BB%9D%E5%AF%B9%E5%94%AF%E5%BF%83%E4%B8%BB%E4%B9%89/"},{"name":"荣格","slug":"荣格","link":"tags/%E8%8D%A3%E6%A0%BC/"},{"name":"当代艺术","slug":"当代艺术","link":"tags/%E5%BD%93%E4%BB%A3%E8%89%BA%E6%9C%AF/"},{"name":"集体潜意识","slug":"集体潜意识","link":"tags/%E9%9B%86%E4%BD%93%E6%BD%9C%E6%84%8F%E8%AF%86/"},{"name":"当代文学","slug":"当代文学","link":"tags/%E5%BD%93%E4%BB%A3%E6%96%87%E5%AD%A6/"}],"categories":[{"name":"NLP","slug":"NLP","link":"categories/NLP/"},{"name":"数学建模","slug":"数学建模","link":"categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"},{"name":"DL","slug":"DL","link":"categories/DL/"},{"name":"python","slug":"python","link":"categories/python/"},{"name":"哲学","slug":"哲学","link":"categories/%E5%93%B2%E5%AD%A6/"},{"name":"社会学","slug":"社会学","link":"categories/%E7%A4%BE%E4%BC%9A%E5%AD%A6/"},{"name":"经济","slug":"经济","link":"categories/%E7%BB%8F%E6%B5%8E/"},{"name":"碎碎念","slug":"碎碎念","link":"categories/%E7%A2%8E%E7%A2%8E%E5%BF%B5/"},{"name":"精神分析","slug":"精神分析","link":"categories/%E7%B2%BE%E7%A5%9E%E5%88%86%E6%9E%90/"},{"name":"心理学","slug":"心理学","link":"categories/%E5%BF%83%E7%90%86%E5%AD%A6/"}],"pages":[]}